nitace/go-lang-app:v6

DAY 33
Docker Swarm        Vs            Kubernetes
No Auto Scaling                   Auto Scaling
No GUI                            GUI
Setup is Easy                     Setup is complex
Good community                    but k8s Great Community support

DOCKER SWARM:
CLUSTER 
NODES
CONTAINER
APP

C: CLUSTER
N: NODE
P: POD
C: CONTAINER
A: APPLICATION

NOTE: k8s dont communicate with containers.
it communicate with pods.

COMPONENTS:
MASTER NODE:
1. API SERVER        : Its for communicating with cluster, it takes command executes and gives output.
2. ETCD              : Its a DB od cluster, all the cluster info will store here.
3. SCHEDULER         : Schedules pods on worker nodes, based on hardware resources. (Master Node assign the pod  to worker based on the  highest hardware capacity)
4. CONTROLLER        : Used to control the k8s objects.
1. cloud controllers : Work for cloud 
2. kube controllers  : Work for on-premise

WORKER NODE:
KUBELET 	  : Its an agent used to communicate with master, tell master node event happen on worker  node
KUBEPROXY  : It deals with nlw.
POD        : Its a group of containers.

There are multiple ways to setup kubernetes cluster.
1.SELF MANAGER K8'S CLUSTER
 a. mini kube (single node cluster) = For only development and testing phase
 b. kubeadm  (multi node cluster)   =  For Creating Production Grade Cluster
 c. KOPS 

2. CLOUD MANAGED K8'S CLUSTER
  a. AWS EKS
  b. AZURE AKS
  c. GCP GKS
  d. IBM IKE

MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
Here Master and worker runs on same machine.
It is a platform Independent.

NOTE: But we dont implement this in real-time

REQUIRMENTS:
2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

PODS:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 
We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)

IMPERATIVE: ()
  kubectl run <pod_name> --image nginx  : Create the pod 
  kubectl get po/pod/pods               : list the running pod
  kubectl get po/pod/pods -o wide       : list the running pod with  more information
  kubectl describe pod <pod_name>       : Show the metadata pod 
  kubectl delete pod <pod_name>         : delete the pod
  kubectl exec -it <pod_name> -- bash   : To go inside the pod  
  kubectl delete pod --all              : delete all pod 

Declarative Method :
vim test.yml

apiVersion: v1
kind: Pod
metadata:
   name: pod1
spec:
  containers:
    - image: nginx
      name: cont1

kubectl create -f test.yml
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod <pod_name>
kubectl delete pod <pod_name>

Note : Conatiner inside the pod share same network or same IP. Usually we do not create multiple container in a single pod. but id we create the multiple container  on same pod then also  all the container shared same IP address.
============================================================
DAY 35:
DRAWBACK:
If we delete the pod we cant retrive.
All the load will be handled by single pod.

REPLICA SET:
it will create same pod of multiple replicas.
if we delete one pod it will create automatically.
we can distribute the load also.


LABLE: assing to a pod for identification. to work with them as single unit.
SELECTOR: used to identify the pod with same label.

kubectl api-resources

REPLICASET:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  labels:
    app: swiggy
  name: swiggy-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: nginx

kubectl create -f abc.yml
kubectl get rs
kubectl get rs -o wide
kubectl describe rs swiggy-rs
kubectl delete rs swiggy-rs
kubectl edit rs/swiggy-rs

SCALING: 
SCALE-IN: Increasing the count of pods
kubectl scale rs/swiggy-rs --replicas=10

SCALE-OUT: Decreasing the count of pods
kubectl scale rs/swiggy-rs --replicas=5

SCALING FOLLOWS LIFO PATTERN:
LIFO: LAST IN FIRST OUT
the pod which is created will be deleted first automatically when we scale out.

DEPLOYMENT:
it will do all operations link RS.
it will do roll back which cannot be done in rs.

rs -- > pods
deployment -- > rs -- > pods

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: nginx

kubectl get deploy
kubectl get deploy -o wide
kubectl describe deploy swiggy-rs
kubectl edit deploy/swiggy-rs
kubectl delete deploy swiggy-r
kubectl delete po --all


KUBECOLOR:

wget https://github.com/hidetatz/kubecolor/releases/download/v0.0.25/kubecolor_0.0.25_Linux_x86_64.tar.gz
tar -zxvf kubecolor_0.0.25_Linux_x86_64.tar.gz
./kubecolor
chmod +x kubecolor
mv kubecolor /usr/local/bin/
kubecolor get po
===========================================================================

There are multiple ways to setup kubernetes cluster.

1.SELF MANAGER K8'S CLUSTER
a.mini kube (single node cluster)
b.kubeadm(multi node cluster)
c. KOPS

2. CLOUD MANAGED K8'S CLUSTER
a. AWS EKS
b.AZURE AKS
c.GCP GKS
d.IBM IKE

MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
Here Master and worker runs on same machine.
It is a platform Independent.

NOTE: But we dont implement this in real-time

REQUIRMENTS:
2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

PODS:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE: 
kubectl run pod1 --image rahamshaik/paytmtrain:latest
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

Declarative:
vim abc.yml

apiVersion: v1
kind: Pod
metadata:
   name: pod1
spec:
  containers:
    - image: nginx
      name: cont1

kubectl create -f abc.yml
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

HISTORY:
  1  apt update -y
    2  apt upgrade -y
    3  sudo apt install curl wget apt-transport-https -y
    4  sudo curl -fsSL https://get.docker.com -o get-docker.sh
    5  ll
    6  sh get-docker.sh
    7  sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
    8  ll
    9  sudo mv minikube-linux-amd64 /usr/local/bin/minikube
   10  chmod +x /usr/local/bin/minikube
   11  minikube version
   12  sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux                                                                                                                                                           /amd64/kubectl"
   13  ll
   14  sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/k                                                                                                                                                           ubectl.sha256"
   15  echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
   16  sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
   17  kubectl version
   18  kubectl version --client
   19  minikube start --driver=docker --force
   20  minikube status
   21  kubectl get pod
   22  kubectl get pods
   23  kubectl get po
   24  kubectl run pod1 --image rahamshaik/paytmtrain:latest
   25  kubectl get pod
   26  kubectl get pods
   27  kubectl get po
   28  kubectl describe pod pod1
   29  kubectl get po
   30  kubectl delete pod pod1
   31  kubectl get po
   32  kubectl run pod1 --image ubuntu
   33  kubectl get po
   34  kubectl delete pod pod1
   35  kubectl run raham --image nginx
   36  kubectl get po
   37  kubectl describe pod raham
   38  kubectl get po -o wide
   39  kubectl delete pod raham
   40  vim abc.yml
   41  kubectl create -f abc.yml
   42  kubectl get po
   43  kubectl get po -o wide
   44  kubectl describe pod pod1
   45  kubectl delete pod pod1
   46  history
===========================================================
DAY 36
KOPS:

INFRASTRUCTURE: Resources used to run our application on cloud.
EX: Ec2, VPC, ALB, -------------

Minikube -- > single node cluster
All the pods on single node 

KOPS, also known as Kubernetes operations.
It is an open-source tool that helps you create, destroy, upgrade, and maintain a highly available, production-grade Kubernetes cluster. 
Depending on the requirement, kOps can also provide cloud infrastructure.
KOPS is mostly used in deploying AWS and GCE Kubernetes clusters. 
But officially, the tool only supports AWS. Support for other cloud providers (such as DigitalOcean, GCP, and OpenStack) are in the beta stage.


ADVANTAGES:
•	Automates the provisioning of AWS and GCE Kubernetes clusters
•	Deploys highly available Kubernetes masters
•	Supports rolling cluster updates
•	Autocompletion of commands in the command line
•	Generates Terraform and CloudFormation configurations
•	Manages cluster add-ons.
•	Supports state-sync model for dry-runs and automatic idempotency
•	Creates instance groups to support heterogeneous clusters

ALTERNATIVES:
Amazon EKS , MINIKUBE, KUBEADM, RANCHER, TERRAFORM.


STEP-1: GIVING PERMISSIONS
i. IAM -- > USER -- > CREATE USER -- > NAME: KOPS -- > Attach Polocies Directly -- > AdministratorAccess -- > NEXT -- > CREATE USER
ii. USER -- > SECURTITY CREDENTIALS -- > CREATE ACCESS KEYS -- > CLI -- > CHECKBOX -- >  CREATE ACCESS KEYS -- > DOWNLOAD 

aws configure

SETP-2: INSTALL KUBECTL AND KOPS

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
mv kubectl /usr/local/bin/kubectl

wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 
mv kops-linux-amd64 /usr/local/bin/kops

vim .bashrc
export PATH=$PATH:/usr/local/bin/  -- > save and exit
source .bashrc

SETP-3: CREATEING BUCKET 
aws s3api create-bucket --bucket devopsbyraham007.k8s.local --region us-east-1
aws s3api put-bucket-versioning --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://cloudanddevopsbyraham007.k8s.local

SETP-4: CREATING THE CLUSTER
kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro
kops update cluster --name rahams.k8s.local --yes --admin


Suggestions:
 * list clusters with:              kops get cluster
 * edit this cluster with:          kops edit cluster rahams.k8s.local
 * edit your node instance group:   kops edit ig --name=rahams.k8s.local nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=rahams.k8s.local master-us-east-1a


ADMIN ACTIVITIES:
To scale the worker nodes:
kops edit ig --name=rahams.k8s.local nodes-us-east-1a
kops update cluster --name rahams.k8s.local --yes --admin 
kops rolling-update cluster --yes

kubectl describe node node_id
kops delete cluster --name rahams.k8s.local --yes

HISTORY:
1  aws configure
    2  curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd6                                                           4/kubectl"
    3  wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
    4  chmod +x kops-linux-amd64 kubectl
    5  mv kubectl /usr/local/bin/kubectl
    6  mv kops-linux-amd64 /usr/local/bin/kops
    7  kops version
    8  kubectl version
    9  vim .bashrc
   10  source .bashrc
   11  kops version
   12  kubectl version
   13  aws s3api create-bucket --bucket devopsbyraham007.k8s.local --region us-east-1
   14  aws s3api create-bucket --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1
   15  aws s3api put-bucket-versioning --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1 --vers                                                           ioning-configuration Status=Enabled
   16  export KOPS_STATE_STORE=s3://cloudanddevopsbyraham007.k8s.local
   17  aws s3api create-bucket --bucket devopsbyraham007.k8s.local --region us-east-1
   18  aws s3api put-bucket-versioning --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1 --vers                                                           ioning-configuration Status=Enabled
   19  export KOPS_STATE_STORE=s3://cloudanddevopsbyraham007.k8s.local
   20  kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medi                                                           um --node-count=2 --node-size t2.micro
   21  kops update cluster --name rahams.k8s.local --yes --admin
   22  kops validate cluster --wait 10m
   23  kops get cluster
   24  kubectl get no
   25  kops edit ig --name=rahams.k8s.local nodes-us-east-1a
   26  kops update cluster --name rahams.k8s.local --yes --admin
   27  kops rolling-update cluster --yes
   28  kops edit ig --name=rahams.k8s.local master-us-east-1a
   29  kops update cluster --name rahams.k8s.local --yes --admin
   30  kops rolling-update cluster --yes
   31  kops edit ig --name=rahams.k8s.local nodes-us-east-1a
   32  kubectl get no
   33  vim abc.yml
   34  kubectl get po
   35  kubectl create -f abc.yml
   36  kubectl get po -o wide
   37  kubectl scale deploy/swiggyrs --replicas=8
   38  kubectl scale deploy/swiggy-rs --replicas=8
   39  kubectl get po -o wide
   40  kubectl describe node i-0c3a6f2269bc5af42
   41  kops get cluster
   42  kops delete cluster --name rahams.k8s.local --yes
   43  history

==================================================================================================================
DAY 37:
NAMESPACES:

NAMESPACE: It is used to divide the cluster to multiple teams on real time.
it is used to isolate the env.

CLUSTER   : HOUSE
NAMESPACES: ROOM

Each namespace is isolated.
if your are room-1 are you able to see room-2.
we cant access the objects from one namespace to another namespace.


TYPES:

default           : Is the default namespace, all objects will create here only
kube-node-lease   : it will store object which is taken from one namespace to another.
kube-public	      : all the public objects will store here.      
kube-system 	    : default k8s will create some objects, those are storing on this ns.

kubectl get pod -n kube-system	: to list all pods in kube-system namespace
kubectl get pod -n default	    : to list all pods in default namespace
kubectl get pod -n kube-public	: to list all pods in kube-public namespace
kubectl get po -A		            : to list all pods in all namespaces

kubectl create ns dev	                                : to create namespace
kubectl config set-context --current --namespace=dev  : to switch to the namespace
kubectl config view --minify | grep namespace         : to see current namespace
kubectl delete ns dev	                                : to delete namespace
kubectl delete pod --all                              : to delete all pods
kubectl delete all --all                              : delete all kubernetes resources of  a namespace
kubectl delete all --all -n {namespace}               : delete all resources of a defined namespace



NOTE: By deleting  the ns all objects also gets deleted.
in real time we use rbac concept to restrict the access from one namespace to another.
so users cant access/delete ns, because of the restriction we provide.
we create roles and rolebind for the users.

------------------------------------------------------------
SERVICE: It is used to expose the application in k8s.


TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        ap p: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: sv1
spec:
  type: ClusterIP
  selector:
    app: swiggy
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.


kubectl get svc,deploy   : get all the service resources and deploy resources
-------------------------------------------
2. NODEPORT: It will expose our application in a particular port.
Range: 30000 - 32767 (in sg we need to give all traffic)

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: NodePort
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80
      nodePort: 31111

NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC & SSH)
DRAWBACK:
PORT RESTRICTION.
-----------------------------------------------------
3. LOADBALACER: It will expose our app and distribute load blw pods.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80
====================================================
#ClusterIP : It will work inside the cluster,

apiVersion: v1
kind: Service
metadata: 
  name: sv1
spec:
  type: ClusterIP
  selector: 
    tier: backend
  ports:
   - port: 4444

---
# NODEPORT = IT WILL EXPOSE THE APPLICATION OUTSIDE THE CLUSTER

apiVersion: v1 
kind: Service
metadata:
  name: sv2 
spec:
  type: NodePort
  selector: 
    tier: backend
  port:
    - port: 4444
      targetPort: 4444
      nodePort: 31111
---
# LOAD BALANCER :
======================================================================

kubernetes Storage :

Application need  storage to store their data 
  1. Ephemeral Storage
      - emptyDir
      - Data lost when pod restarts
  2. Persistant Storage 
      - EBS, EFS
      - Persist data when pod restarts
  
  
